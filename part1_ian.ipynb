{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "import json\n",
    "import imutils\n",
    "import argparse\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cv2HOGDescriptor = cv2.HOGDescriptor((64,64), (16,16), (8,8), (8,8), 9, 1, -1, 0, 0.2, 1, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSubImage(img, bbox):\n",
    "    top = round(bbox[\"top\"])\n",
    "    left = round(bbox[\"left\"])\n",
    "    right = round(bbox[\"right\"])\n",
    "    bottom = round(bbox[\"bottom\"])\n",
    "    \n",
    "    width = right - left\n",
    "    height = bottom - top\n",
    "    diff = width - height\n",
    "    add = round(diff/2)\n",
    "\n",
    "    subImg = img[top - add:bottom + add, left:right]\n",
    "    return subImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    " \n",
    "# Returns true if two rectangles(l1, r1)\n",
    "# and (l2, r2) overlap\n",
    "def doOverlap(l1, r1, l2, r2):\n",
    "     \n",
    "    # To check if either rectangle is actually a line\n",
    "      # For example  :  l1 ={-1,0}  r1={1,1}  l2={0,-1}  r2={0,1}\n",
    "       \n",
    "    if (l1.x == r1.x or l1.y == r2.y or l2.x == r2.x or l2.y == r2.y):\n",
    "        # the line cannot have positive overlap\n",
    "        return False\n",
    "     \n",
    "    # If one rectangle is on left side of other\n",
    "    if(l1.x >= r2.x or l2.x >= r1.x):\n",
    "        return False\n",
    " \n",
    "    # If one rectangle is above other\n",
    "    if(l1.y <= r2.y or l2.y <= r1.y):\n",
    "        return False\n",
    " \n",
    "    return True\n",
    "    \n",
    "#https://www.geeksforgeeks.org/find-two-rectangles-overlap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_crop(image, crop_height, crop_width, bboxes):\n",
    "    max_x = image.shape[1] - crop_width\n",
    "    max_y = image.shape[0] - crop_height\n",
    "    x = np.random.randint(0, max_x)\n",
    "    y = np.random.randint(0, max_y)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        l1 = Point(x,y)\n",
    "        r1 = Point(max_x, max_y)\n",
    "        l2 = Point(bbox['left'], bbox['top'])\n",
    "        r2 = Point(bbox['right'], bbox['bottom'])\n",
    "\n",
    "        if (doOverlap(l1,r1,l2,r2)):\n",
    "            return False, []\n",
    "\n",
    "    crop = image[y: y + crop_height, x: x + crop_width]\n",
    "    return True, crop\n",
    "\n",
    "#https://stackoverflow.com/questions/42263020/opencv-trying-to-get-random-portion-of-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetFeatureVector(n, test=False,debug=False):\n",
    "    number = str(n)\n",
    "    dataType = \"\"\n",
    "    if test: \n",
    "        dataType = \"test\"\n",
    "    else: \n",
    "        dataType = \"train\"\n",
    "\n",
    "    if debug:\n",
    "        print(dataType)\n",
    "\n",
    "    training_path = \"benchmark_velocity_\" + dataType + \"/clips/\" + number + \"/imgs/040.jpg\"\n",
    "    annotation = \"benchmark_velocity_\" + dataType + \"/clips/\" + number + \"/annotation.json\"\n",
    "    cars = []\n",
    "    nonCars = []\n",
    "    with open(annotation) as jsonFile:\n",
    "        x = json.load(jsonFile)\n",
    "        img = cv2.imread(training_path)\n",
    "        bboxes = [i[\"bbox\"] for i in x]\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            s = GetSubImage(img, bbox)\n",
    "            resized_img = cv2.resize(s, (64, 64))\n",
    "            fd = cv2HOGDescriptor.compute(resized_img)[:,0]\n",
    "            cars.append(fd.tolist())\n",
    "            \n",
    "            #Note: feature extraction using skimage hog performs much slower than cv2\n",
    "            #fd, hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
    "            #    \tcells_per_block=(2, 2), visualize=True, multichannel=True)  \n",
    "\n",
    "            if (debug):\n",
    "                plt.imshow(resized_img)\n",
    "                plt.show()\n",
    "    \n",
    "            if not test:\n",
    "                randomCount = 0\n",
    "                v, c = get_random_crop(img, s.shape[0], s.shape[1], bboxes)\n",
    "                randImages = []\n",
    "                while (randomCount < 5):\n",
    "                    if v:\n",
    "                        randomCount += 1\n",
    "                        randImages.append(c)\n",
    "                    v,c = get_random_crop(img, s.shape[0], s.shape[1], bboxes)\n",
    "        \n",
    "                for croppedImage in randImages:\n",
    "                    resized_cimg = cv2.resize(croppedImage, (64, 64))\n",
    "\n",
    "                    #Note: feature extraction using skimage hog performs much slower than cv2\n",
    "                    #fd, hog_image = hog(resized_cimg, orientations=9, pixels_per_cell=(8, 8),\n",
    "                \t#cells_per_block=(2, 2), visualize=True, multichannel=True)\n",
    "\n",
    "                    fd = cv2HOGDescriptor.compute(resized_cimg)[:,0]\n",
    "                    nonCars.append(fd.tolist())\n",
    "        \n",
    "        if test:\n",
    "            return cars\n",
    "        \n",
    "        return [cars, nonCars] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSuppFeatureVectors():   \n",
    "    path = \"benchmark_velocity_supp\"\n",
    "    cars = []\n",
    "    nonCars = []\n",
    "    annotation = path + \"/annotation.json\"\n",
    "\n",
    "    with open(annotation) as jsonFile:\n",
    "        x = json.load(jsonFile)\n",
    "        for dictionary in x[:1000]:\n",
    "            name = dictionary[\"file_name\"]\n",
    "            bboxes = dictionary[\"bbox\"]\n",
    "            img = cv2.imread(path + \"/\" + name)\n",
    "\n",
    "            for bbox in bboxes:\n",
    "                s = GetSubImage(img, bbox)\n",
    "                try:\n",
    "                    resized_img = cv2.resize(s, (64, 64))\n",
    "                except:\n",
    "                    #labelled car is too small\n",
    "                    #print(\"error car is too small to detect\")\n",
    "                    #print(bbox)\n",
    "                    continue\n",
    "                \n",
    "                fd = cv2HOGDescriptor.compute(resized_img)[:,0]      \n",
    "                cars.append(fd)\n",
    "                randomCount = 0\n",
    "                v, c = get_random_crop(img, s.shape[0], s.shape[1], bboxes)\n",
    "                randImages = []\n",
    "                tries = 0\n",
    "\n",
    "                while (randomCount < 20 and tries < 10):\n",
    "                    if v:\n",
    "                        randomCount += 1\n",
    "                        randImages.append(c)\n",
    "                    else:\n",
    "                        tries += 1\n",
    "\n",
    "                    v,c = get_random_crop(img, s.shape[0], s.shape[1], bboxes)\n",
    "                \n",
    "                for croppedImage in randImages:\n",
    "                    resized_cimg = cv2.resize(croppedImage, (64, 64))\n",
    "                    fd = cv2HOGDescriptor.compute(resized_cimg)[:,0]   \n",
    "                    nonCars.append(fd)\n",
    "\n",
    "    return [cars, nonCars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "getting Feature Vectors from supplmentary dataset\n",
      "getting Feature Vectors from training dataset\n",
      "scaling training data\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.69892984, -0.25487555,  0.38395966, ..., -0.94795683,\n",
       "        -0.17379206, -0.58598213],\n",
       "       [-0.76992863,  0.61840467,  1.42714125, ..., -1.11763876,\n",
       "        -1.04256756, -0.76317799],\n",
       "       [-0.33367209, -0.07800933, -0.77800717, ..., -1.05178834,\n",
       "        -0.99495611, -0.54369203],\n",
       "       ...,\n",
       "       [-0.77482577,  0.05518334,  1.34771871, ..., -0.22430778,\n",
       "        -0.86920411, -0.70652897],\n",
       "       [-0.15400843, -0.97635951, -1.03431817, ...,  0.90295506,\n",
       "        -0.81712132, -0.85366977],\n",
       "       [-0.74911532,  0.67490717,  2.82824088, ...,  1.12899754,\n",
       "         0.54258317, -0.40969199]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "training_images = []\n",
    "training_labels = []\n",
    "\n",
    "print(\"getting Feature Vectors from supplmentary dataset\")\n",
    "retVal = GetSuppFeatureVectors()\n",
    "cars = retVal[0]\n",
    "nonCars = retVal[1]\n",
    "\n",
    "for c in cars:\n",
    "    training_images.append(c)\n",
    "    training_labels.append(0)\n",
    "\n",
    "for nc in nonCars:\n",
    "    training_images.append(nc)\n",
    "    training_labels.append(1)\n",
    "\n",
    "print(\"getting Feature Vectors from training dataset\")\n",
    "for i in range(1, 1075):\n",
    "    retVal = GetFeatureVector(i, False, False)\n",
    "    cars = retVal[0]\n",
    "    nonCars = retVal[1]\n",
    "\n",
    "    for c in cars:\n",
    "        training_images.append(c)\n",
    "        training_labels.append(0)\n",
    "    \n",
    "    for nc in nonCars:\n",
    "        training_images.append(nc)\n",
    "        training_labels.append(1)\n",
    "\n",
    "print('scaling training data')\n",
    "scaler.fit(training_images)\n",
    "scaler.transform(training_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "getting Feature Vectors from test dataset\n",
      "scaling test data\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.1138388 , -0.17764873, -0.59076364, ..., -0.45454964,\n",
       "        -1.04181115, -0.85053914],\n",
       "       [ 0.39919657, -0.49866178, -0.76469145, ..., -1.09909699,\n",
       "        -1.02598774, -0.63961623],\n",
       "       [-0.36977486,  1.28306984,  1.66021691, ..., -1.1113548 ,\n",
       "        -0.98894316, -0.77668866],\n",
       "       ...,\n",
       "       [-0.39631239,  1.45426161,  1.21426095, ..., -1.11788195,\n",
       "        -1.04274516, -0.83710891],\n",
       "       [-0.5064983 ,  1.82406646,  0.54504553, ..., -1.07725584,\n",
       "        -1.01811304, -0.84719316],\n",
       "       [-0.65831106,  0.8975717 , -0.21605323, ..., -1.09156837,\n",
       "        -0.9724385 , -0.49724428]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "print(\"getting Feature Vectors from test dataset\")\n",
    "for i in range(1, 270):\n",
    "    cars = GetFeatureVector(i, True)\n",
    "    for c in cars:\n",
    "        test_images.append(c)\n",
    "        test_labels.append(0)\n",
    "\n",
    "print('scaling test data')\n",
    "scaler.transform(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "svm made\n",
      "fitted\n",
      "scored\n",
      "predicting\n",
      "EVALUATION ON TESTING DATA\n",
      "[[363  12]\n",
      " [  0   0]]\n",
      "0.968\n",
      "0.9841151738473167\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(dual=False)\n",
    "print(\"svm made\")\n",
    "svm = svm.fit(training_images, training_labels)\n",
    "print(\"fitted\")\n",
    "score = svm.score(test_images, test_labels)\n",
    "print(\"scored\")\n",
    "\n",
    "print(\"predicting\")\n",
    "svmpred = svm.predict(test_images)\n",
    "svmpred2 = svm.predict(training_images)\n",
    "\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "\n",
    "print(confusion_matrix(test_labels, svmpred))\n",
    "print(metrics.accuracy_score(test_labels, svmpred))\n",
    "print(metrics.accuracy_score(training_labels, svmpred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "\t# slide a window across the image\n",
    "\tfor y in range(0, image.shape[0], stepSize):\n",
    "\t\tfor x in range(0, image.shape[1], stepSize):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n",
    "\n",
    "#https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBoxes(image, classifier):\n",
    "# construct the argument parser and parse the arguments\n",
    "    windowSizes = [(32,32), (64,64), (128, 128), (256,256), (512, 512)]\n",
    "    bboxes = []\n",
    "    \n",
    "    numRows = image.shape[0]\n",
    "    third = int(numRows/3)\n",
    "    croppedImg = image[third:]\n",
    "\n",
    "    for windowSize in windowSizes:\n",
    "        (winW, winH) = windowSize\n",
    "        for (x, y, window) in sliding_window(croppedImg, stepSize=10, windowSize=(winW, winH)):\n",
    "            # if the window does not meet our desired window size, ignore it\n",
    "            if window.shape[0] != winH or window.shape[1] != winW:\n",
    "                continue\n",
    "\n",
    "            # THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A\n",
    "            # MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE\n",
    "            # WINDOW\n",
    "            resized_img = cv2.resize(window, (64, 64))\n",
    "\n",
    "            fd  = cv2HOGDescriptor.compute(resized_img)[:,0]\n",
    "            \n",
    "            result = classifier.predict([fd])\n",
    "            #print(result)\n",
    "            if result[0] == 0:\n",
    "                #print(\"car\")\n",
    "                #plt.imshow(window)\n",
    "                #plt.show()\n",
    "                bbox = {\"top\": y + third, \"left\": x, \"right\": x + winW, \"bottom\": y + winH + third}\n",
    "                bboxes.append(bbox)\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "    #https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "\t# if there are no boxes, return an empty list\n",
    "\tif len(boxes) == 0:\n",
    "\t\treturn []\n",
    "\t# if the bounding boxes integers, convert them to floats --\n",
    "\t# this is important since we'll be doing a bunch of divisions\n",
    "\tif boxes.dtype.kind == \"i\":\n",
    "\t\tboxes = boxes.astype(\"float\")\n",
    "\t# initialize the list of picked indexes\t\n",
    "\tpick = []\n",
    "\t# grab the coordinates of the bounding boxes\n",
    "\tx1 = boxes[:,0]\n",
    "\ty1 = boxes[:,1]\n",
    "\tx2 = boxes[:,2]\n",
    "\ty2 = boxes[:,3]\n",
    "\t# compute the area of the bounding boxes and sort the bounding\n",
    "\t# boxes by the bottom-right y-coordinate of the bounding box\n",
    "\tarea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\tidxs = np.argsort(y2)\n",
    "\t# keep looping while some indexes still remain in the indexes\n",
    "\t# list\n",
    "\twhile len(idxs) > 0:\n",
    "\t\t# grab the last index in the indexes list and add the\n",
    "\t\t# index value to the list of picked indexes\n",
    "\t\tlast = len(idxs) - 1\n",
    "\t\ti = idxs[last]\n",
    "\t\tpick.append(i)\n",
    "\t\t# find the largest (x, y) coordinates for the start of\n",
    "\t\t# the bounding box and the smallest (x, y) coordinates\n",
    "\t\t# for the end of the bounding box\n",
    "\t\txx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "\t\tyy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "\t\txx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "\t\tyy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\t\t# compute the width and height of the bounding box\n",
    "\t\tw = np.maximum(0, xx2 - xx1 + 1)\n",
    "\t\th = np.maximum(0, yy2 - yy1 + 1)\n",
    "\t\t# compute the ratio of overlap\n",
    "\t\toverlap = (w * h) / area[idxs[:last]]\n",
    "\t\t# delete all indexes from the index list that have\n",
    "\t\tidxs = np.delete(idxs, np.concatenate(([last],\n",
    "\t\t\tnp.where(overlap > overlapThresh)[0])))\n",
    "\t# return only the bounding boxes that were picked using the\n",
    "\t# integer data type\n",
    "\treturn boxes[pick].astype(\"int\")\n",
    "\n",
    "    #https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "creating output dir\n",
      "benchmark_velocity_test/clips/1/imgs/040.jpg\n",
      "benchmark_velocity_test/clips/2/imgs/040.jpg\n",
      "benchmark_velocity_test/clips/3/imgs/040.jpg\n",
      "benchmark_velocity_test/clips/4/imgs/040.jpg\n",
      "benchmark_velocity_test/clips/5/imgs/040.jpg\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-46ceedf7db67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m270\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mGetBoxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-46ceedf7db67>\u001b[0m in \u001b[0;36mGetBoxes\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"benchmark_velocity_test/clips/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/imgs/040.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateBoxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mboxList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d263622dbff2>\u001b[0m in \u001b[0;36mcreateBoxes\u001b[0;34m(image, classifier)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mresized_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mfd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcv2HOGDescriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresized_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def GetBoxes(n):\n",
    "    test_path = \"benchmark_velocity_test/clips/\" + str(n) + \"/imgs/040.jpg\"\n",
    "    img = cv2.imread(test_path)\n",
    "    bboxes = createBoxes(img, svm)\n",
    "    boxList = []\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        boxList.append((bbox[\"left\"], bbox[\"top\"], bbox[\"right\"], bbox[\"bottom\"]))\n",
    "    \n",
    "    boundingBoxes = np.array(boxList)\n",
    "    pick = non_max_suppression_fast(boundingBoxes, 0.3)\n",
    "    \n",
    "    for (startX, startY, endX, endY) in pick:\n",
    "\t    cv2.rectangle(img, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    if (not os.path.exists('output')):\n",
    "        print('creating output dir')\n",
    "        os.mkdir('./output')\n",
    "\n",
    "    cv2.imwrite(\"output/output\" + str(n) + \".jpg\", img)\n",
    "    print(test_path)\n",
    "\n",
    "for i in range(1,270):\n",
    "    GetBoxes(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}